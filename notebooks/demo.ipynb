{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Step 1: Clone the PEFT library and install dependencies\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11b07b07ac5e472b"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-22T09:24:06.811342Z",
     "start_time": "2024-10-22T09:24:06.215196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'peft' already exists and is not an empty directory.\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: pip: command not found\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'status': 'ok', 'restart': True}"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!git clone https://github.com/tsachiblau/peft_CPT.git peft\n",
    "!pip install -e ./peft\n",
    "\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 2: Import libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3dd6c123d091a87"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import CPTConfig, get_peft_model\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Union, Any, Dict\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "tokenizer_name_or_path = 'bigscience/bloom-1b7'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-22T09:24:18.594134Z",
     "start_time": "2024-10-22T09:24:13.523515Z"
    }
   },
   "id": "69232e530bcb9f7b",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 3: Load the Tokenizer and Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d261acf3a959fb7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_name_or_path,\n",
    "    cache_dir='.',\n",
    "    padding_side='right',\n",
    "    trust_remote_code=True\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-22T09:24:46.026016Z",
     "start_time": "2024-10-22T09:24:44.448283Z"
    }
   },
   "id": "c472bb6389ef031c",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 4: Preprocess dataset with string labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a3d90722835517d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset('glue', 'sst2')\n",
    "\n",
    "def add_string_labels(example):\n",
    "    example['label_text'] = \"positive\" if example['label'] == 1 else \"negative\"\n",
    "    return example\n",
    "\n",
    "train_dataset = dataset['train'].select(range(4)).map(add_string_labels)\n",
    "test_dataset = dataset['validation'].select(range(20)).map(add_string_labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-22T09:24:53.077591Z",
     "start_time": "2024-10-22T09:24:49.857943Z"
    }
   },
   "id": "8045f83c4ba4360e",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 5: Define the CPT Dataset Class\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2970baaa6382ac86"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 728.56it/s]\n"
     ]
    }
   ],
   "source": [
    "class CPTDataset(Dataset):\n",
    "    def __init__(self, samples, tokenizer, template, max_length=MAX_INPUT_LENGTH):\n",
    "        self.template = template\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.attention_mask = []\n",
    "        self.input_ids = []\n",
    "        self.input_type_mask = []\n",
    "        self.inter_seperator_ids = self._get_input_ids(template['inter_seperator'])\n",
    "\n",
    "        for sample_i in tqdm(samples):\n",
    "            input_text, label = sample_i['sentence'], sample_i['label_text']\n",
    "            input_ids, attention_mask, input_type_mask = self.preprocess_sentence(input_text, label)\n",
    "\n",
    "            self.input_ids.append(input_ids)\n",
    "            self.attention_mask.append(attention_mask)\n",
    "            self.input_type_mask.append(input_type_mask)\n",
    "\n",
    "    def _get_input_ids(self, text):\n",
    "        return self.tokenizer(text, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    def preprocess_sentence(self, input_text, label):\n",
    "        input_template_part_1_text, input_template_part_2_text = self.template['input'].split('{}')\n",
    "        input_template_tokenized_part1 = self._get_input_ids(input_template_part_1_text)\n",
    "        input_tokenized = self._get_input_ids(input_text)\n",
    "        input_template_tokenized_part2 = self._get_input_ids(input_template_part_2_text)\n",
    "\n",
    "        sep_tokenized = self._get_input_ids(self.template['intra_seperator'])\n",
    "\n",
    "        label_template_part_1, label_template_part_2 = self.template['output'].split('{}')\n",
    "        label_template_part1_tokenized = self._get_input_ids(label_template_part_1)\n",
    "        label_tokenized = self._get_input_ids(label)\n",
    "        label_template_part2_tokenized = self._get_input_ids(label_template_part_2)\n",
    "\n",
    "        eos = [self.tokenizer.eos_token_id] if self.tokenizer.eos_token_id is not None else []\n",
    "        input_ids = input_template_tokenized_part1 + input_tokenized + input_template_tokenized_part2 + sep_tokenized + label_template_part1_tokenized + label_tokenized + label_template_part2_tokenized + eos\n",
    "\n",
    "        # determine label tokens, to calculate loss only over them when labels_loss == True\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        input_type_mask = [1] * len(input_template_tokenized_part1) + [2] * len(input_tokenized) + [1] * len(\n",
    "            input_template_tokenized_part2) + [0] * len(sep_tokenized) + \\\n",
    "                          [3] * len(label_template_part1_tokenized) + [4] * len(label_tokenized) + [3] * len( \\\n",
    "            label_template_part2_tokenized) + [0] * len(eos)\n",
    "\n",
    "        assert len(input_type_mask) == len(input_ids) == len(attention_mask)\n",
    "\n",
    "        return input_ids, attention_mask, input_type_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"input_type_mask\": self.input_type_mask[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "templates = {\n",
    "    'input': 'input: {}',\n",
    "    'intra_seperator': ' ',\n",
    "    'output': 'output: {}',\n",
    "    'inter_seperator': '\\n'\n",
    "}\n",
    "\n",
    "CPT_train_dataset = CPTDataset(train_dataset, tokenizer, templates)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-22T09:24:53.989024Z",
     "start_time": "2024-10-22T09:24:53.962405Z"
    }
   },
   "id": "73b3e1c53ebf790f",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 6: Create Context"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2abec20291f869"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 [8684, 29, 210, 41587, 2084, 9999, 940, 1485, 368, 123643, 32643, 210, 210, 19308, 29, 210, 111017, 2, 8684, 29, 210, 43453, 654, 40501, 630, 3804, 12378, 376, 380, 13430, 210, 210, 19308, 29, 210, 111017, 2, 8684, 29, 210, 19562, 141046, 3776, 26702, 530, 14016, 2893, 7747, 17303, 40704, 3638, 7384, 9670, 210, 210, 19308, 29, 210, 96675, 2, 8684, 29, 210, 3842, 7849, 105708, 999, 85816, 427, 17398, 368, 5025, 36387, 210, 210, 19308, 29, 210, 111017, 2]\n",
      "80 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "80 [1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 3, 3, 3, 4, 0, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 7, 7, 7, 8, 0, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 0, 11, 11, 11, 12, 0, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 0, 15, 15, 15, 16, 0]\n"
     ]
    }
   ],
   "source": [
    "context_ids = []\n",
    "context_attention_mask = []\n",
    "context_input_type_mask = []\n",
    "first_type_mask = 0\n",
    "\n",
    "for i in range(len(CPT_train_dataset)):\n",
    "    context_ids += CPT_train_dataset[i]['input_ids']\n",
    "    context_attention_mask += CPT_train_dataset[i]['attention_mask']\n",
    "    context_input_type_mask += [i + first_type_mask if i > 0 else 0 for i in CPT_train_dataset[i]['input_type_mask']]\n",
    "    first_type_mask += 4\n",
    "\n",
    "print(len(context_ids), context_ids)\n",
    "print(len(context_attention_mask), context_attention_mask)\n",
    "print(len(context_input_type_mask), context_input_type_mask)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-22T09:24:58.894814Z",
     "start_time": "2024-10-22T09:24:58.893841Z"
    }
   },
   "id": "aef03bbd5d86d3d8",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 7: Load Base Model and Configure CPT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c40f24774d83372"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'bigscience/bloom-1b7',\n",
    "    cache_dir='.',\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=False,\n",
    ")\n",
    "\n",
    "config = CPTConfig(\n",
    "            CPT_token_ids=context_ids,\n",
    "            CPT_mask=context_attention_mask,\n",
    "            CPT_tokens_type_mask=context_input_type_mask,\n",
    "            CPT_prompt_tuning_init=\"TEXT\",\n",
    "            num_virtual_tokens=len(context_ids),\n",
    "\n",
    "            opt_weighted_loss_type='decay',\n",
    "            opt_loss_decay_factor=0.95,\n",
    "            opt_projection_epsilon=0.2,\n",
    "            opt_projection_format_epsilon=0.1,\n",
    "\n",
    "            tokenizer_name_or_path=tokenizer_name_or_path,\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, config)## load training data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-22T09:25:08.941945Z",
     "start_time": "2024-10-22T09:25:04.393323Z"
    }
   },
   "id": "17ac445134919a39",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 8: Configuring Collate Function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e49660c50d98741"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CPTDataCollatorForLanguageModeling(DataCollatorForLanguageModeling):\n",
    "    def __init__(self, tokenizer, training=True, mlm=False):\n",
    "        super().__init__(tokenizer, mlm=mlm)\n",
    "        self.training = training\n",
    "        self.tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})  # mk check why needed\n",
    "\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        list_sample_mask = []\n",
    "        for i in range(len(examples)):\n",
    "            if \"sample_mask\" in examples[i].keys():\n",
    "                list_sample_mask.append(examples[i].pop(\"sample_mask\"))\n",
    "\n",
    "        max_len = max(len(ex[\"input_ids\"]) for ex in examples)\n",
    "\n",
    "        def pad_sequence(sequence, max_len, pad_value=0):\n",
    "            return sequence + [pad_value] * (max_len - len(sequence))\n",
    "\n",
    "        input_ids = torch.tensor([pad_sequence(ex[\"input_ids\"], max_len) for ex in examples])\n",
    "        attention_mask = torch.tensor([pad_sequence(ex[\"attention_mask\"], max_len) for ex in examples])\n",
    "        input_type_mask = torch.tensor([pad_sequence(ex[\"input_type_mask\"], max_len) for ex in examples])\n",
    "\n",
    "        batch = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"input_type_mask\": input_type_mask}\n",
    "\n",
    "        tensor_sample_mask = batch[\"input_ids\"].clone().long()\n",
    "        tensor_sample_mask[:, :] = 0\n",
    "        for i in range(len(list_sample_mask)):\n",
    "            tensor_sample_mask[i, : len(list_sample_mask[i])] = list_sample_mask[i]\n",
    "\n",
    "        batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
    "        if not self.training:\n",
    "            batch[\"sample_mask\"] = tensor_sample_mask\n",
    "\n",
    "        return batch\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-22T09:25:08.953199Z",
     "start_time": "2024-10-22T09:25:08.945689Z"
    }
   },
   "id": "b0fac840f060e3aa",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 9: Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48f535d74e6602b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsachiblau/miniconda3/envs/cpt_public/lib/python3.12/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/100 : < :, Epoch 0.25/25]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=100, training_loss=0.6106603369908408, metrics={'train_runtime': 12.1518, 'train_samples_per_second': 8.229, 'train_steps_per_second': 8.229, 'total_flos': 14503280640000.0, 'train_loss': 0.6106603369908408, 'epoch': 25.0})"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='../.',  # Where the model predictions and checkpoints will be written\n",
    "    use_cpu=False,  # This is necessary for CPU clusters.\n",
    "    auto_find_batch_size=False,  # Find a suitable batch size that will fit into memory automatically\n",
    "    learning_rate=1e-4,  # Higher learning rate than full Fine-Tuning\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    save_total_limit=1,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=25,\n",
    "    fp16=True,\n",
    "    save_strategy='no'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  # We pass in the PEFT version of the foundation model, bloomz-560M\n",
    "    args=training_args,  # The args for the training.\n",
    "    train_dataset=CPT_train_dataset,  # The dataset used to train the model.\n",
    "    data_collator=CPTDataCollatorForLanguageModeling(tokenizer, training=True, mlm=False)\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-22T09:25:27.599132Z",
     "start_time": "2024-10-22T09:25:13.906685Z"
    }
   },
   "id": "1a865c2ad2dc7218",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 10: Evaluate the Model\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b799ea89a567590f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 971.09it/s]\n",
      "Using `past_key_values` as a tuple is deprecated and will be removed in v4.45. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: input: it 's a charming and often affecting journey .  output: positive</s> \n",
      " \t The prediction is: positive\n",
      " \t The GT is positive\n",
      "Sentence: input: unflinchingly bleak and desperate  output: negative</s> \n",
      " \t The prediction is: negative\n",
      " \t The GT is negative\n",
      "Sentence: input: allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker .  output: positive</s> \n",
      " \t The prediction is: positive\n",
      " \t The GT is positive\n",
      "Sentence: input: the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales .  output: positive</s> \n",
      " \t The prediction is: positive\n",
      " \t The GT is positive\n",
      "Sentence: input: it 's slow -- very , very slow .  output: negative</s> \n",
      " \t The prediction is: negative\n",
      " \t The GT is negative\n",
      "Sentence: input: although laced with humor and a few fanciful touches , the film is a refreshingly serious look at young women .  output: positive</s> \n",
      " \t The prediction is: negative\n",
      " \t The GT is positive\n",
      "Sentence: input: a sometimes tedious film .  output: negative</s> \n",
      " \t The prediction is: negative\n",
      " \t The GT is negative\n",
      "Sentence: input: or doing last year 's taxes with your ex-wife .  output: negative</s> \n",
      " \t The prediction is: negative\n",
      " \t The GT is negative\n",
      "Sentence: input: you do n't have to know about music to appreciate the film 's easygoing blend of comedy and romance .  output: positive</s> \n",
      " \t The prediction is: positive\n",
      " \t The GT is positive\n",
      "Sentence: input: in exactly 89 minutes , most of which passed as slowly as if i 'd been sitting naked on an igloo , formula 51 sank from quirky to jerky to utter turkey .  output: negative</s> \n",
      " \t The prediction is: negative\n",
      " \t The GT is negative\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_dataset = test_dataset.select_columns(['sentence', 'label_text'])\n",
    "CPT_test_dataset = CPTDataset(test_dataset, tokenizer, templates)\n",
    "device = model.device\n",
    "\n",
    "for i in range(10):\n",
    "    input_ids, input_type_mask = CPT_test_dataset[i]['input_ids'], CPT_test_dataset[i]['input_type_mask']\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=torch.Tensor(input_ids).long().to(device=device).view(1, -1),\n",
    "        labels=torch.Tensor(input_ids).long().to(device=device).view(1, -1),\n",
    "        input_type_mask=torch.Tensor(input_type_mask).long().to(device=device).view(1, -1)\n",
    "    )\n",
    "\n",
    "    shifted_logits = outputs.logits[..., :-1, :].contiguous().to(model.dtype)[0, -len(input_ids) + 1:]\n",
    "    shift_labels = torch.Tensor(input_ids).long().to(device=device).view(1, -1)[0, 1:].contiguous().to(device)\n",
    "    shifted_input_type_mask = torch.Tensor(input_type_mask).long().to(device=device).view(1, -1)[..., 1:].contiguous().to(device)\n",
    "\n",
    "    mask = torch.Tensor(shifted_input_type_mask).long().to(device=device).view(-1,) == 4\n",
    "    logit = shifted_logits[mask]\n",
    "    label = shift_labels[mask]\n",
    "    all_labels = torch.Tensor([tokenizer(i, add_special_tokens=False)[\"input_ids\"] for i in ['negative', 'positive']]).long().to(device).view(-1,)\n",
    "\n",
    "    prediction = logit[0, torch.Tensor([tokenizer(i, add_special_tokens=False)[\"input_ids\"] for i in ['negative', 'positive']]).long().to(device).view(-1,)].argmax()\n",
    "    prediction_text = 'negative' if prediction == 0 else 'positive'\n",
    "    print('Sentence: {} \\n \\t The prediction is: {}\\n \\t The GT is {}'.format(tokenizer.decode(input_ids), prediction_text, tokenizer.decode(label)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-22T09:25:28.252009Z",
     "start_time": "2024-10-22T09:25:27.598326Z"
    }
   },
   "id": "48e7d976e6e01212",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "580805cb69ebec26"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
